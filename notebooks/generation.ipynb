{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add upper folder to syspath\n",
    "import sys\n",
    "sys.path.append('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "from synthlume.metrics.ssi import SentenceSeparabilityIndex\n",
    "from synthlume.metrics.cosine_similarity import CosineSimilarity\n",
    "from synthlume.metrics.gmm_wasserstain import GMMWasserstein\n",
    "from synthlume.pipeline.step import (\n",
    "    DescriptionStep,\n",
    "    GenerateQuestionStep,\n",
    "    HumanifyQuestionStep,\n",
    "    ScenarioQuestionStep,\n",
    "    QuestionStyleSimpleStep,\n",
    "    QuestionStyleCompleteSentenseStep,\n",
    "    MultipleChoiceQuestionStep\n",
    ")\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get env variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "## constants\n",
    "AZURE_OPENAI_KEY = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "AZURE_DEPLOYMENT_NAME = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
    "AZURE_ENDPOINT=os.getenv(\"AZURE_ENDPOINT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f\"data/sample_texts.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=256,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([data])\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=AZURE_OPENAI_KEY,\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    openai_api_version=\"2023-08-01-preview\",\n",
    "    deployment_name=AZURE_DEPLOYMENT_NAME,\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "description_step = DescriptionStep(llm=llm, language=\"en\")\n",
    "\n",
    "description = description_step.generate(document=data[:2048])\n",
    "description = description[description_step.output_key]\n",
    "\n",
    "\"\"\"questions_generatoion_step = GenerateQuestionStep(llm=llm, language=\"en\")\n",
    "multiple_choice_step = MultipleChoiceQuestionStep(llm=llm, language=\"en\")\n",
    "humanify_question_step = HumanifyQuestionStep(llm=llm, language=\"en\")\n",
    "scenario_question_step = ScenarioQuestionStep(llm=llm, language=\"en\")\n",
    "\n",
    "pipe = questions_generatoion_step | (multiple_choice_step & humanify_question_step & scenario_question_step)\n",
    "\n",
    "res = pipe.generate(\n",
    "    context=texts[0].page_content,\n",
    "    description=description,\n",
    ")\n",
    "\n",
    "print(res)\n",
    "\n",
    "exit(0)\"\"\"\n",
    "\n",
    "questions_generatoion_step = GenerateQuestionStep(llm=llm, language=\"en\")\n",
    "scenario_question_step = ScenarioQuestionStep(llm=llm, language=\"en\")\n",
    "humanify_question_step = HumanifyQuestionStep(llm=llm, language=\"en\")\n",
    "question_style_simple_step = QuestionStyleSimpleStep(llm=llm, language=\"en\")\n",
    "complete_sentence_step = QuestionStyleCompleteSentenseStep(llm=llm, language=\"en\")\n",
    "multiple_choice_step = MultipleChoiceQuestionStep(llm=llm, language=\"en\")\n",
    "\n",
    "results = []\n",
    "\n",
    "output_jsonl = open(\"data/output.jsonl\", \"w\")\n",
    "\n",
    "for i, chunk in enumerate(texts[:2]):\n",
    "    chunk = chunk.page_content\n",
    "    print(f\"Chunk {i+1}/{len(texts)}\")\n",
    "    calls = {}\n",
    "\n",
    "    inputs = {\n",
    "        \"context\": chunk,\n",
    "        \"description\": description\n",
    "    }\n",
    "\n",
    "    calls[\"input\"] = inputs\n",
    "\n",
    "    response = questions_generatoion_step.generate(**inputs)\n",
    "\n",
    "    if response is None:\n",
    "        print(f\"Could not generate question, skipping\")\n",
    "        continue\n",
    "\n",
    "    calls[questions_generatoion_step.name] = response\n",
    "    print(f\"Base generated question: {calls[questions_generatoion_step.name]['question']}\")\n",
    "\n",
    "    response = multiple_choice_step.generate(**calls[questions_generatoion_step.name])\n",
    "    if response is None:\n",
    "        print(f\"Could not generate multiple choice question, skipping\")\n",
    "    else:\n",
    "        calls[multiple_choice_step.name] = response\n",
    "        print(f\"Multiple choice generated question: {calls[multiple_choice_step.name]['question']}\")\n",
    "        print(f\"\\tA) {calls[multiple_choice_step.name]['answer']}\")\n",
    "        print(f\"\\tB) {calls[multiple_choice_step.name]['wrong_answer_1']}\")\n",
    "        print(f\"\\tC) {calls[multiple_choice_step.name]['wrong_answer_2']}\")\n",
    "        print(f\"\\tD) {calls[multiple_choice_step.name]['wrong_answer_3']}\")\n",
    "\n",
    "    response = scenario_question_step.generate(**calls[questions_generatoion_step.name])\n",
    "    if response is None:\n",
    "        print(f\"Could not generate scenario question, skipping\")\n",
    "    else:\n",
    "        calls[scenario_question_step.name] = response\n",
    "        print(f\"Scenario generated question: {calls[scenario_question_step.name]['question']}\")\n",
    "\n",
    "    response = humanify_question_step.generate(**calls[questions_generatoion_step.name])\n",
    "    if response is None:\n",
    "        print(f\"Could not generate human-like question, skipping\")\n",
    "    else:\n",
    "        calls[humanify_question_step.name] = response\n",
    "        print(f\"Human-like generated question: {calls[humanify_question_step.name]['question']}\")\n",
    "\n",
    "    response = question_style_simple_step.generate(**calls[questions_generatoion_step.name])\n",
    "    if response is None:\n",
    "        print(f\"Could not generate simple question, skipping\")\n",
    "    else:\n",
    "        calls[question_style_simple_step.name] = response\n",
    "        print(f\"Simple generated question: {calls[question_style_simple_step.name]['question']}\")\n",
    "\n",
    "    response = complete_sentence_step.generate(**calls[questions_generatoion_step.name])\n",
    "    if response is None:\n",
    "        print(f\"Could not generate complete sentence question, skipping\")\n",
    "    else:\n",
    "        calls[complete_sentence_step.name] = response\n",
    "        print(f\"Complete sentence generated question: {calls[complete_sentence_step.name]['question']}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    output_jsonl.write(json.dumps(calls) + \"\\n\")\n",
    "\n",
    "    results.append(calls)\n",
    "\n",
    "data = []\n",
    "for call in results:\n",
    "    row = {}\n",
    "    row[\"context\"] = call[\"input\"][\"context\"]\n",
    "    for name in [\n",
    "        questions_generatoion_step.name,\n",
    "        scenario_question_step.name,\n",
    "        humanify_question_step.name,\n",
    "        question_style_simple_step.name,\n",
    "        complete_sentence_step.name,\n",
    "    ]:\n",
    "        if name not in call:\n",
    "            continue\n",
    "        row[f\"{name}_question\"] = call[name][\"question\"]\n",
    "        row[f\"{name}_answer\"] = call[name][\"answer\"]\n",
    "\n",
    "    if multiple_choice_step.name not in call:\n",
    "        continue\n",
    "    row[f\"{multiple_choice_step.name}_question\"] = call[multiple_choice_step.name][\"question\"]\n",
    "    row[f\"{multiple_choice_step.name}_answer\"] = call[multiple_choice_step.name][\"answer\"]\n",
    "    row[f\"{multiple_choice_step.name}_wrong_answer_1\"] = call[multiple_choice_step.name][\"wrong_answer_1\"]\n",
    "    row[f\"{multiple_choice_step.name}_wrong_answer_2\"] = call[multiple_choice_step.name][\"wrong_answer_2\"]\n",
    "    row[f\"{multiple_choice_step.name}_wrong_answer_3\"] = call[multiple_choice_step.name][\"wrong_answer_3\"]\n",
    "\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"data/questions.csv\", index=False)\n",
    "\n",
    "with open(\"data/questions.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synthlume",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
